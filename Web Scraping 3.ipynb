{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "053c7490",
   "metadata": {},
   "source": [
    "## 1. Write a python program which searches all the product under a particular product from www.amazon.in. The product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for guitars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a830f9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Necessary Library\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time\n",
    "from time import sleep\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "358f924b",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\Users\\Thamizh\\Downloads\\chromedriver_win32 (2)\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ab39babf",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('http://www.amazon.in/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8cc8a950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the element to be search: Guitar\n"
     ]
    }
   ],
   "source": [
    "input_user = input('Enter the element to be search: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "332359a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_bar = driver.find_element_by_id('twotabsearchtextbox')\n",
    "search_bar.send_keys(input_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "58a21914",
   "metadata": {},
   "outputs": [],
   "source": [
    "srch = driver.find_element_by_id('nav-search-submit-button')\n",
    "srch.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b1aaea",
   "metadata": {},
   "source": [
    "## 2. In the above question, now scrape the following details of each product listed in first 3 pages of your search results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then scrape all the products available under that product name. Details to be scraped are: \"Brand Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and “Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1fca35fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=[]\n",
    "for i in driver.find_elements_by_xpath('//a[@class=\"a-link-normal s-link-style a-text-normal\"]'):\n",
    "    url.append(i.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "372fd24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_name_ = []\n",
    "rating = []\n",
    "price = []\n",
    "avail = []\n",
    "no_rating=[]\n",
    "exc=[]\n",
    "brand_name=[]\n",
    "color=[]\n",
    "other=[]\n",
    "delivery=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c391e7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "77b7797c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running this code first time to fetch the data from first page(url)\n",
    "for i in url: \n",
    "    driver.get(i)\n",
    "    time.sleep(2)\n",
    "    try:\n",
    "        p=driver.find_element_by_id('productTitle')\n",
    "        prod_name_.append(p.text.replace('/n',''))\n",
    "    except NoSuchElementException:\n",
    "        prod_name_.append('-')\n",
    "    try:\n",
    "        pr=driver.find_element_by_id('priceblock_ourprice')\n",
    "        price.append(pr.text.replace('/n',''))\n",
    "    except NoSuchElementException:\n",
    "        price.append('-')\n",
    "    try:\n",
    "        r=driver.find_element_by_xpath(\"//span[@class='a-size-medium a-color-base']\")\n",
    "        rating.append(r.text.replace('/n',''))\n",
    "    except NoSuchElementException:\n",
    "        rating.append('-')\n",
    "    try:\n",
    "        r=driver.find_element_by_id(\"availability\")\n",
    "        avail.append(r.text.replace('/n',''))\n",
    "    except NoSuchElementException:\n",
    "        avail.append('-')\n",
    "    try:\n",
    "        r=driver.find_element_by_id(\"acrCustomerReviewText\")\n",
    "        no_rating.append(r.text.replace('/n',''))\n",
    "    except NoSuchElementException:\n",
    "        no_rating.append('-')\n",
    "    try:\n",
    "        r=driver.find_element_by_xpath(\"//table[@class='a-normal a-spacing-micro']/tbody/tr/td[2]/span\")\n",
    "        brand_name.append(r.text.replace('/n',''))\n",
    "    except NoSuchElementException:\n",
    "        brand_name.append('-')\n",
    "    try:\n",
    "        r=driver.find_element_by_xpath(\"//div[@class='a-section a-spacing-none icon-content'][1]/a\")\n",
    "        exc.append(r.text.replace('/n',''))\n",
    "    except NoSuchElementException:\n",
    "        exc.append('-')\n",
    "    try:\n",
    "        r=driver.find_element_by_xpath(\"//table[@class='a-normal a-spacing-micro']/tbody/tr[2]/td[2]/span\")\n",
    "        color.append(r.text.replace('/n',''))\n",
    "    except NoSuchElementException:\n",
    "        color.append('-')\n",
    "    try:\n",
    "        r=driver.find_element_by_id(\"productDescription\")\n",
    "        other.append(r.text.replace('/n',''))\n",
    "    except NoSuchElementException:\n",
    "        other.append('-')\n",
    "    try:\n",
    "        r=driver.find_element_by_xpath(\"//div[@class='a-section a-spacing-mini'][1]/b\")\n",
    "        delivery.append(r.text.replace('/n',''))\n",
    "    except NoSuchElementException:\n",
    "        delivery.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "98f7a736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After fetching the  data of first page the last product url is on the browser so again directing to first page.\n",
    "srch = driver.find_element_by_id('nav-search-submit-button')\n",
    "srch.click() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fce65e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving from first page to second page.\n",
    "nxt = driver.find_element_by_xpath(\"//a[@class='s-pagination-item s-pagination-next s-pagination-button s-pagination-separator']\")\n",
    "nxt.click() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "efc16d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_1=[]\n",
    "for i in driver.find_elements_by_xpath('//a[@class=\"a-link-normal s-link-style a-text-normal\"]'):\n",
    "    url_1.append(i.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e0b2bd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running this code second time to fetch the data from second page(url_1)\n",
    "for i in url_1: \n",
    "    driver.get(i)\n",
    "    time.sleep(2)\n",
    "    try:\n",
    "        p=driver.find_element_by_id('productTitle')\n",
    "        prod_name_.append(p.text.replace('/n',''))\n",
    "    except NoSuchElementException:\n",
    "        prod_name_.append('-')\n",
    "    try:\n",
    "        pr=driver.find_element_by_id('priceblock_ourprice')\n",
    "        price.append(pr.text.replace('/n',''))\n",
    "    except NoSuchElementException:\n",
    "        price.append('-')\n",
    "    try:\n",
    "        r=driver.find_element_by_xpath(\"//span[@class='a-size-medium a-color-base']\")\n",
    "        rating.append(r.text.replace('/n',''))\n",
    "    except NoSuchElementException:\n",
    "        rating.append('-')\n",
    "    try:\n",
    "        r=driver.find_element_by_id(\"availability\")\n",
    "        avail.append(r.text.replace('/n',''))\n",
    "    except NoSuchElementException:\n",
    "        avail.append('-')\n",
    "    try:\n",
    "        r=driver.find_element_by_id(\"acrCustomerReviewText\")\n",
    "        no_rating.append(r.text.replace('/n',''))\n",
    "    except NoSuchElementException:\n",
    "        no_rating.append('-')\n",
    "    try:\n",
    "        r=driver.find_element_by_xpath(\"//table[@class='a-normal a-spacing-micro']/tbody/tr/td[2]/span\")\n",
    "        brand_name.append(r.text.replace('/n',''))\n",
    "    except NoSuchElementException:\n",
    "        brand_name.append('-')\n",
    "    try:\n",
    "        r=driver.find_element_by_xpath(\"//div[@class='a-section a-spacing-none icon-content'][1]/a\")\n",
    "        exc.append(r.text.replace('/n',''))\n",
    "    except NoSuchElementException:\n",
    "        exc.append('-')\n",
    "    try:\n",
    "        r=driver.find_element_by_xpath(\"//table[@class='a-normal a-spacing-micro']/tbody/tr[2]/td[2]/span\")\n",
    "        color.append(r.text.replace('/n',''))\n",
    "    except NoSuchElementException:\n",
    "        color.append('-')\n",
    "    try:\n",
    "        r=driver.find_element_by_id(\"productDescription\")\n",
    "        other.append(r.text.replace('/n',''))\n",
    "    except NoSuchElementException:\n",
    "        other.append('-')\n",
    "    try:\n",
    "        r=driver.find_element_by_xpath(\"//div[@class='a-section a-spacing-mini'][1]/b\")\n",
    "        delivery.append(r.text.replace('/n',''))\n",
    "    except NoSuchElementException:\n",
    "        delivery.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "642cddcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After fetching the  data of second page the last product url is on the browser so again directing to first page.\n",
    "srch = driver.find_element_by_id('nav-search-submit-button')\n",
    "srch.click() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "47b299c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For moving from first to third page we need to run this code twice.\n",
    "nxt = driver.find_element_by_xpath(\"//a[@class='s-pagination-item s-pagination-next s-pagination-button s-pagination-separator']\")\n",
    "nxt.click() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "fb102a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_2=[]\n",
    "for i in driver.find_elements_by_xpath('//a[@class=\"a-link-normal s-link-style a-text-normal\"]'):\n",
    "    url_2.append(i.get_attribute('href'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4e49aeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running this code third time to fetch the data from third page(url_2)\n",
    "for i in url_2:\n",
    "    driver.get(i)\n",
    "    time.sleep(2)\n",
    "    try:\n",
    "        p=driver.find_element_by_id('productTitle')\n",
    "        prod_name_.append(p.text.replace('/n',''))\n",
    "    except NoSuchElementException:\n",
    "        prod_name_.append('-')\n",
    "    try:\n",
    "        pr=driver.find_element_by_id('priceblock_ourprice')\n",
    "        price.append(pr.text.replace('/n',''))\n",
    "    except NoSuchElementException:\n",
    "        price.append('-')\n",
    "    try:\n",
    "        r=driver.find_element_by_xpath(\"//span[@class='a-size-medium a-color-base']\")\n",
    "        rating.append(r.text.replace('/n',''))\n",
    "    except NoSuchElementException:\n",
    "        rating.append('-')\n",
    "    try:\n",
    "        r=driver.find_element_by_id(\"availability\")\n",
    "        avail.append(r.text.replace('/n',''))\n",
    "    except NoSuchElementException:\n",
    "        avail.append('-')\n",
    "    try:\n",
    "        r=driver.find_element_by_id(\"acrCustomerReviewText\")\n",
    "        no_rating.append(r.text.replace('/n',''))\n",
    "    except NoSuchElementException:\n",
    "        no_rating.append('-')\n",
    "    try:\n",
    "        r=driver.find_element_by_xpath(\"//table[@class='a-normal a-spacing-micro']/tbody/tr/td[2]/span\")\n",
    "        brand_name.append(r.text.replace('/n',''))\n",
    "    except NoSuchElementException:\n",
    "        brand_name.append('-')\n",
    "    try:\n",
    "        r=driver.find_element_by_xpath(\"//div[@class='a-section a-spacing-none icon-content'][1]/a\")\n",
    "        exc.append(r.text.replace('/n',''))\n",
    "    except NoSuchElementException:\n",
    "        exc.append('-')\n",
    "    try:\n",
    "        r=driver.find_element_by_xpath(\"//table[@class='a-normal a-spacing-micro']/tbody/tr[2]/td[2]/span\")\n",
    "        color.append(r.text.replace('/n',''))\n",
    "    except NoSuchElementException:\n",
    "        color.append('-')\n",
    "    try:\n",
    "        r=driver.find_element_by_id(\"productDescription\")\n",
    "        other.append(r.text.replace('/n',''))\n",
    "    except NoSuchElementException:\n",
    "        other.append('-')\n",
    "    try:\n",
    "        r=driver.find_element_by_xpath(\"//div[@class='a-section a-spacing-mini'][1]/b\")\n",
    "        delivery.append(r.text.replace('/n',''))\n",
    "    except NoSuchElementException:\n",
    "        delivery.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2fbe7a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending urls of second and third page to first list. \n",
    "url.extend(url_1)   \n",
    "url.extend(url_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "67edd8d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the length of our url.\n",
    "len(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9c340c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making DataFrame\n",
    "amazon_guitar = pd.DataFrame({})\n",
    "amazon_guitar['Brand Name'] = brand_name\n",
    "amazon_guitar['Product Name'] = prod_name_\n",
    "amazon_guitar['Price'] = price\n",
    "amazon_guitar['Rating'] = rating\n",
    "amazon_guitar['No of ratings'] = no_rating\n",
    "amazon_guitar['Delivery'] = delivery\n",
    "amazon_guitar['Availability'] = avail\n",
    "amazon_guitar['Exchange'] = exc\n",
    "amazon_guitar['Color'] = color\n",
    "amazon_guitar['Product_url'] = url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2ac79713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>No of ratings</th>\n",
       "      <th>Delivery</th>\n",
       "      <th>Availability</th>\n",
       "      <th>Exchange</th>\n",
       "      <th>Color</th>\n",
       "      <th>Product_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Brand Name, Product Name, Price, Rating, No of ratings, Delivery, Availability, Exchange, Color, Product_url]\n",
       "Index: []"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_guitar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6022bb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_guitar = amazon_guitar.to_csv(index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0509fd20",
   "metadata": {},
   "source": [
    "## 3. Write a python program to access the search bar and search button on images.google.com and scrape 10images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0e59af38",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://images.google.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "65e05197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function which takes the necessary input as argument\n",
    "def get_urls(keywords):\n",
    "        driver.get('https://images.google.com')\n",
    "\n",
    "        # Locating the search bar and click search button\n",
    "        search_bar=driver.find_element_by_xpath(\"//input[@class='gLFyf gsfi']\")\n",
    "        search_bar.send_keys(keywords)\n",
    "        button=driver.find_element_by_xpath(\"//button[@class='Tg7LZd']\")\n",
    "        button.click()\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # loading few pages to scrape images\n",
    "        for k in range(10):\n",
    "            driver.find_element_by_xpath(\"//a[@class='wXeWr islib nfEiy']\").send_keys(Keys.PAGE_DOWN)\n",
    "        time.sleep(5)\n",
    "        picture = [image for image in driver.find_elements_by_xpath(\"//a[@class='wXeWr islib nfEiy']\")[:10]]\n",
    "        image_urls=[]\n",
    "        print(len(picture))\n",
    "        if(len(picture)==10):\n",
    "            for i in picture:\n",
    "                try:\n",
    "                    i.click()\n",
    "                    image_urls.append(i.get_attribute('href'))\n",
    "                except:\n",
    "                    image_urls.append('Not Available')\n",
    "            return image_urls\n",
    "        else:\n",
    "            print('Image Not Available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "8afac725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "#Let's scrape the url's of all images\n",
    "fruits = get_urls('Fruits')\n",
    "cars = get_urls('Cars')\n",
    "machine_learning = get_urls('Machine Learning')\n",
    "guitar = get_urls('Guitar')\n",
    "cake = get_urls('Cakes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "848719ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fruits</th>\n",
       "      <th>Cars</th>\n",
       "      <th>Machine Learning</th>\n",
       "      <th>Guitar</th>\n",
       "      <th>Cakes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "      <td>https://www.google.com/imgres?imgurl=https%3A%...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Fruits  \\\n",
       "0  https://www.google.com/imgres?imgurl=https%3A%...   \n",
       "1  https://www.google.com/imgres?imgurl=https%3A%...   \n",
       "2  https://www.google.com/imgres?imgurl=https%3A%...   \n",
       "3  https://www.google.com/imgres?imgurl=https%3A%...   \n",
       "4  https://www.google.com/imgres?imgurl=https%3A%...   \n",
       "5  https://www.google.com/imgres?imgurl=https%3A%...   \n",
       "6  https://www.google.com/imgres?imgurl=https%3A%...   \n",
       "7  https://www.google.com/imgres?imgurl=https%3A%...   \n",
       "8  https://www.google.com/imgres?imgurl=https%3A%...   \n",
       "9  https://www.google.com/imgres?imgurl=https%3A%...   \n",
       "\n",
       "                                                Cars  \\\n",
       "0  https://www.google.com/imgres?imgurl=https%3A%...   \n",
       "1  https://www.google.com/imgres?imgurl=https%3A%...   \n",
       "2  https://www.google.com/imgres?imgurl=https%3A%...   \n",
       "3  https://www.google.com/imgres?imgurl=https%3A%...   \n",
       "4  https://www.google.com/imgres?imgurl=https%3A%...   \n",
       "5  https://www.google.com/imgres?imgurl=https%3A%...   \n",
       "6  https://www.google.com/imgres?imgurl=https%3A%...   \n",
       "7  https://www.google.com/imgres?imgurl=https%3A%...   \n",
       "8  https://www.google.com/imgres?imgurl=https%3A%...   \n",
       "9  https://www.google.com/imgres?imgurl=https%3A%...   \n",
       "\n",
       "                                    Machine Learning  \\\n",
       "0  https://www.google.com/imgres?imgurl=https%3A%...   \n",
       "1  https://www.google.com/imgres?imgurl=https%3A%...   \n",
       "2  https://www.google.com/imgres?imgurl=https%3A%...   \n",
       "3  https://www.google.com/imgres?imgurl=https%3A%...   \n",
       "4  https://www.google.com/imgres?imgurl=https%3A%...   \n",
       "5  https://www.google.com/imgres?imgurl=https%3A%...   \n",
       "6  https://www.google.com/imgres?imgurl=https%3A%...   \n",
       "7  https://www.google.com/imgres?imgurl=https%3A%...   \n",
       "8  https://www.google.com/imgres?imgurl=https%3A%...   \n",
       "9  https://www.google.com/imgres?imgurl=https%3A%...   \n",
       "\n",
       "                                              Guitar  \\\n",
       "0  https://www.google.com/imgres?imgurl=https%3A%...   \n",
       "1  https://www.google.com/imgres?imgurl=https%3A%...   \n",
       "2  https://www.google.com/imgres?imgurl=https%3A%...   \n",
       "3  https://www.google.com/imgres?imgurl=https%3A%...   \n",
       "4  https://www.google.com/imgres?imgurl=https%3A%...   \n",
       "5  https://www.google.com/imgres?imgurl=https%3A%...   \n",
       "6  https://www.google.com/imgres?imgurl=https%3A%...   \n",
       "7  https://www.google.com/imgres?imgurl=https%3A%...   \n",
       "8  https://www.google.com/imgres?imgurl=https%3A%...   \n",
       "9  https://www.google.com/imgres?imgurl=https%3A%...   \n",
       "\n",
       "                                               Cakes  \n",
       "0  https://www.google.com/imgres?imgurl=https%3A%...  \n",
       "1  https://www.google.com/imgres?imgurl=https%3A%...  \n",
       "2  https://www.google.com/imgres?imgurl=https%3A%...  \n",
       "3  https://www.google.com/imgres?imgurl=https%3A%...  \n",
       "4  https://www.google.com/imgres?imgurl=https%3A%...  \n",
       "5  https://www.google.com/imgres?imgurl=https%3A%...  \n",
       "6  https://www.google.com/imgres?imgurl=https%3A%...  \n",
       "7  https://www.google.com/imgres?imgurl=https%3A%...  \n",
       "8  https://www.google.com/imgres?imgurl=https%3A%...  \n",
       "9  https://www.google.com/imgres?imgurl=https%3A%...  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's create a DataFrame for our data\n",
    "Image_URL = pd.DataFrame({})\n",
    "Image_URL['Fruits'] = fruits\n",
    "Image_URL['Cars'] = cars\n",
    "Image_URL['Machine Learning'] = machine_learning\n",
    "Image_URL['Guitar'] = guitar\n",
    "Image_URL['Cakes'] = cake\n",
    "\n",
    "Image_URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f4f540",
   "metadata": {},
   "source": [
    "## 4. Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) \n",
    "www.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be\n",
    "scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”,\n",
    "“Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the\n",
    "details is missing then replace it by “- “. Save your results in a dataframe and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e01bd6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.flipkart.com/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0428f474",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Searching the web element for input\n",
    "search = driver.find_element_by_xpath('//input[@title=\"Search for products, brands and more\"]')\n",
    "search.send_keys('smartphone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d6e77b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locating the search button using xpath\n",
    "search_btn = driver.find_element_by_xpath('//button[@class=\"L0Z3Pu\"]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "7974016e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no data\n"
     ]
    }
   ],
   "source": [
    "# Clicking on search button\n",
    "\n",
    "try:\n",
    "    search_btn.click()\n",
    "except:\n",
    "    print('no data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "6e9fff10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating empty lists to store the fetched data\n",
    "product_urls = []\n",
    "brand_name = []\n",
    "smartphone_name = []\n",
    "smartphone_color = []\n",
    "RAM = []\n",
    "ROM = []\n",
    "primary_camera = []\n",
    "secondary_camera = []\n",
    "display_size = []\n",
    "display_resolution = []\n",
    "processor = []\n",
    "processor_core = []\n",
    "battery_capacity = []\n",
    "price = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "91ac2192",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's define a function.\n",
    "link = driver.find_elements_by_xpath('//a[@class=\"_1fQZEK\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "1d7d5cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's scrape product URL\n",
    "for i in link:\n",
    "    product_urls.append(i.get_attribute('href'))\n",
    "for i in product_urls:\n",
    "    driver.get(i)\n",
    "    driver.implicitly_wait(4)\n",
    "    driver.find_element_by_xpath('//button[@class=\"_2KpZ6l _1FH0tX\"]').click()\n",
    "    try:\n",
    "        \n",
    "#Let's scrape brand name\n",
    "        title=driver.find_element_by_xpath('//span[@class=\"B_NuCI\"]').text.split()\n",
    "        brand_name.append(title[0])\n",
    "    except:\n",
    "        brand_name.append('-')\n",
    "    try:\n",
    "        \n",
    "#Let's scrape smartohone name\n",
    "        smartphone_name.append(driver.find_element_by_xpath('//div[@class=\"_1UhVsV\"]/div[1]/table/tbody/tr[3]/td[2]').text)\n",
    "    except:\n",
    "        smartphone_name.append('-')\n",
    "    try:\n",
    "        \n",
    "#Let's scrape smartphone colour\n",
    "        smartphone_color.append(driver.find_element_by_xpath('//div[@class=\"_1UhVsV\"]/div[1]/table/tbody/tr[4]/td[2]').text)\n",
    "    except:\n",
    "        smartphone_color.append('-')\n",
    "    try:\n",
    "        \n",
    "#Let's scrape smartphone RAM\n",
    "        RAM.append(driver.find_element_by_xpath('//div[@class=\"_1UhVsV\"]/div[4]/table/tbody/tr[2]/td[2]').text)\n",
    "    except:\n",
    "        RAM.append('-')\n",
    "    try:\n",
    "        \n",
    "#Let's scrape smartphone ROM\n",
    "        ROM.append(driver.find_element_by_xpath('//div[@class=\"_1UhVsV\"]/div[4]/table/tbody/tr[1]/td[2]').text)\n",
    "    except:\n",
    "        ROM.append('-')\n",
    "    try:\n",
    "        \n",
    "#Let's scrape smartphone primary camera details\n",
    "        primary_camera.append(driver.find_element_by_xpath('//div[@class=\"_1UhVsV\"]/div[5]/table/tbody/tr[2]/td[2]').text)\n",
    "    except:\n",
    "        primary_camera.append('-')\n",
    "    try:\n",
    "        \n",
    "#Let's scrape smartphone secondary camera details\n",
    "        secondary_camera.append(driver.find_element_by_xpath('//div[@class=\"_1UhVsV\"]/div[5]/table/tbody/tr[5]/td[2]').text)\n",
    "    except:\n",
    "        secondary_camera.append('-')\n",
    "    try:\n",
    "        \n",
    "#Let's scrape smartphone display size\n",
    "        display_size.append(driver.find_element_by_xpath('//div[@class=\"_1UhVsV\"]/div[2]/table/tbody/tr[1]/td[2]').text)\n",
    "    except:\n",
    "        display_size.append('-')\n",
    "    try:\n",
    "        \n",
    "#Let's scrape smartphone screen resolution\n",
    "        display_resolution.append(driver.find_element_by_xpath('//div[@class=\"_1UhVsV\"]/div[2]/table/tbody/tr[2]/td[2]').text)\n",
    "    except:\n",
    "        display_resolution.append('-')\n",
    "    try:\n",
    "        \n",
    "#Let's scrape smartphone processor\n",
    "        processor.append(driver.find_element_by_xpath('//div[@class=\"_1UhVsV\"]/div[3]/table/tbody/tr[2]/td[2]').text)\n",
    "    except:\n",
    "        processor.append('-')\n",
    "    try:\n",
    "        \n",
    "#Let's scrape smartphone processor core\n",
    "        processor_core.append(driver.find_element_by_xpath('//div[@class=\"_1UhVsV\"]/div[3]/table/tbody/tr[3]/td[2]').text)\n",
    "    except:\n",
    "        processor_core.append('-')\n",
    "    try:\n",
    "        \n",
    "#Let's scrape smartphone battery capacity\n",
    "        battery_capacity.append(driver.find_element_by_xpath('//div[@class=\"_1UhVsV\"]/div[10]/table/tbody/tr[1]/td[2]').text)\n",
    "    except:\n",
    "        battery_capacity.append('-')\n",
    "    try:\n",
    "        \n",
    "#Let's scrape smartphone price\n",
    "        price.append(driver.find_element_by_xpath('//div[@class=\"dyC4hf\"]/div/div/div').text)\n",
    "    except:\n",
    "        price.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "4adc8063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Smartphone Name</th>\n",
       "      <th>Smartphone Colour</th>\n",
       "      <th>RAM</th>\n",
       "      <th>Storage (ROM)</th>\n",
       "      <th>Primray Camera</th>\n",
       "      <th>Secondary Camera</th>\n",
       "      <th>Display Size</th>\n",
       "      <th>Display Resolution</th>\n",
       "      <th>Processor</th>\n",
       "      <th>Processor Cores</th>\n",
       "      <th>Battery Capacity</th>\n",
       "      <th>Price</th>\n",
       "      <th>Product URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Brand Name, Smartphone Name, Smartphone Colour, RAM, Storage (ROM), Primray Camera, Secondary Camera, Display Size, Display Resolution, Processor, Processor Cores, Battery Capacity, Price, Product URL]\n",
       "Index: []"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a DATA FRAME to store fetched data\n",
    "Flipkart=pd.DataFrame({})\n",
    "Flipkart['Brand Name'] = brand_name\n",
    "Flipkart['Smartphone Name'] = smartphone_name\n",
    "Flipkart['Smartphone Colour'] = smartphone_color\n",
    "Flipkart['RAM'] = RAM\n",
    "Flipkart['Storage (ROM)'] = ROM\n",
    "Flipkart['Primray Camera'] = primary_camera\n",
    "Flipkart['Secondary Camera'] = secondary_camera\n",
    "Flipkart['Display Size'] = display_size\n",
    "Flipkart['Display Resolution'] = display_resolution\n",
    "Flipkart['Processor'] = processor\n",
    "Flipkart['Processor Cores'] = processor_core\n",
    "Flipkart['Battery Capacity'] = battery_capacity\n",
    "Flipkart['Price'] = price\n",
    "Flipkart['Product URL'] = product_urls\n",
    "\n",
    "Flipkart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "bb426f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the dataframe in csv file\n",
    "Flipkart.to_csv('Flipkart_smartphone.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa7ca1d",
   "metadata": {},
   "source": [
    "### 5. Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on googlemaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "060cee3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.google.com/maps/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "698b316e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter City Name : Bareilly\n"
     ]
    }
   ],
   "source": [
    "#Creating a function to enter the city name \n",
    "city = input('Enter City Name : ') \n",
    "driver.find_element_by_id('searchboxinput').send_keys(city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ad6b47fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latitude: 12.9204224 Longitude: 80.1570816\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Scraping the entered city latitude and Longitude from google map\n",
    "lat = driver.current_url.split('/@')[1].split(',')[0]\n",
    "long = driver.current_url.split('/@')[1].split(',')[1].split(',')[0]\n",
    "print('Latitude:', lat, 'Longitude:', long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98d3429",
   "metadata": {},
   "source": [
    "## 6. Write a program to scrap details of all the funding deals for second quarter (i.e Jan 21 – March 21)from trak.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e69dbd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://trak.in/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "64ecda4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entering the details in the search bar\n",
    "search_button = driver.find_element_by_xpath('//li[@id=\"menu-item-51510\"]/a').get_attribute('href')\n",
    "driver.get(search_button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6f1c5859",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = []\n",
    "for i in driver.find_elements_by_xpath('//td[@class = \"column-2\"]')[:29]:\n",
    "    date.append(i.text.replace('/n',''))\n",
    "    \n",
    "#performed indexing for each column to fetch the data between January 21 – March 21\n",
    "date = date[5:]\n",
    "startup_name = []\n",
    "for j in driver.find_elements_by_xpath('//td[@class = \"column-3\"]')[:29]:\n",
    "    startup_name.append(j.text.replace('/n',''))\n",
    "startup_name = startup_name[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c722bc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "industry = []\n",
    "for j in driver.find_elements_by_xpath('//td[@class = \"column-4\"]')[:29]:\n",
    "    industry.append(j.text.replace('/n',''))\n",
    "industry = industry[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "846f6fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "subvertical = []\n",
    "for j in driver.find_elements_by_xpath('//td[@class = \"column-5\"]')[:29]:\n",
    "    subvertical.append(j.text.replace('/n',''))\n",
    "subvertical = subvertical[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6ae3c51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = []\n",
    "for j in driver.find_elements_by_xpath('//td[@class = \"column-6\"]')[:29]:\n",
    "    loc.append(j.text.replace('/n',''))\n",
    "loc = loc[5:]\n",
    "\n",
    "investor_name = []\n",
    "for j in driver.find_elements_by_xpath('//td[@class=\"column-7\"]')[:29]:\n",
    "    investor_name.append(j.text.replace('/n',''))\n",
    "investor_name = investor_name[5:]\n",
    "\n",
    "investment_type = []\n",
    "for j in driver.find_elements_by_xpath('//td[@class=\"column-8\"]')[:29]:\n",
    "    investment_type.append(j.text.replace('/n',''))\n",
    "investment_type = investment_type[5:]\n",
    "\n",
    "amt = []\n",
    "for j in driver.find_elements_by_xpath('//td[@class=\"column-9\"]')[:29]:\n",
    "    amt.append(j.text.replace('/n',''))\n",
    "amt = amt[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "41c6aaba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Startup name</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Subvertical</th>\n",
       "      <th>Location</th>\n",
       "      <th>Investor name</th>\n",
       "      <th>Investment tpye</th>\n",
       "      <th>Amount(in USD)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>04/03/2021</td>\n",
       "      <td>DealShare</td>\n",
       "      <td>E-commerce</td>\n",
       "      <td>Online shopping platform</td>\n",
       "      <td>Jaipur, Rajasthan</td>\n",
       "      <td>Innoven Capital</td>\n",
       "      <td>Debt Financing</td>\n",
       "      <td>250,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31/03/2021</td>\n",
       "      <td>Uniphore</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Conversational Service Automation (CSA)</td>\n",
       "      <td>Palo Alto</td>\n",
       "      <td>Sorenson Capital Partners</td>\n",
       "      <td>Series D</td>\n",
       "      <td>140,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30/03/2021</td>\n",
       "      <td>Dunzo</td>\n",
       "      <td>E-commerce</td>\n",
       "      <td>Hyper-local delivery app</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Krishtal Advisors Pte Ltd</td>\n",
       "      <td>Series E</td>\n",
       "      <td>8,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30/03/2021</td>\n",
       "      <td>BYJU’S</td>\n",
       "      <td>Edu-tech</td>\n",
       "      <td>Online tutoring</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>MC Global Edtech, B Capital, Baron, others</td>\n",
       "      <td>Series F</td>\n",
       "      <td>460,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23/03/2021</td>\n",
       "      <td>SkilloVilla</td>\n",
       "      <td>Edu-tech</td>\n",
       "      <td>Career and job-oriented upskilling.</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Titan Capital, others</td>\n",
       "      <td>Seed</td>\n",
       "      <td>300,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25/03/2021</td>\n",
       "      <td>CityMall</td>\n",
       "      <td>E-commerce</td>\n",
       "      <td>Social ecommerce and online grocery platform</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>Accel Partners</td>\n",
       "      <td>Series A</td>\n",
       "      <td>11,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>26/03/2021</td>\n",
       "      <td>DotPe</td>\n",
       "      <td>FinTech</td>\n",
       "      <td>Commerce and payments platform to offline ente...</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>PayU</td>\n",
       "      <td>Series A</td>\n",
       "      <td>27,500,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11/02/2021</td>\n",
       "      <td>Doubtnut</td>\n",
       "      <td>Edu Tech</td>\n",
       "      <td>E-Learning Platform</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>SIG Global, Sequoia Capital, WaterBridge Ventu...</td>\n",
       "      <td>Series B</td>\n",
       "      <td>2,500,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>22/02/2021</td>\n",
       "      <td>Zomato</td>\n",
       "      <td>Hospitality</td>\n",
       "      <td>Online Food Delivery Platform</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>Tiger Global, Kora</td>\n",
       "      <td>Venture</td>\n",
       "      <td>250,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>19/02/2021</td>\n",
       "      <td>Fingerlix</td>\n",
       "      <td>Hospitality</td>\n",
       "      <td>Semi-cooked food delivery app</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Rhodium Trust, Accel Partners and Swiggy</td>\n",
       "      <td>Series C</td>\n",
       "      <td>2,747,045.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>17/02/2021</td>\n",
       "      <td>Zolve</td>\n",
       "      <td>FinTech</td>\n",
       "      <td>Global Neobank Venture</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Accel Partners and Lightspeed Venture Partners</td>\n",
       "      <td>Seed</td>\n",
       "      <td>1,50,00,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>15/02/2021</td>\n",
       "      <td>KreditBee</td>\n",
       "      <td>Finance</td>\n",
       "      <td>Digital lending platform</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Azim Premji’s PremjiInvest and South Korea’s M...</td>\n",
       "      <td>Series C</td>\n",
       "      <td>75,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12/02/2021</td>\n",
       "      <td>Pepperfry</td>\n",
       "      <td>E-commerce</td>\n",
       "      <td>Multi-brand furniture brand</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>InnoVen Capital</td>\n",
       "      <td>Debt Financing</td>\n",
       "      <td>4,773,958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12/02/2021</td>\n",
       "      <td>Grofers</td>\n",
       "      <td>E-Commerce</td>\n",
       "      <td>Online supermarket</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>SoftBank Vision Fund (SVF)</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>55,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>09/02/2021</td>\n",
       "      <td>Nothing</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Consumer Technology Venture</td>\n",
       "      <td>London</td>\n",
       "      <td>GV</td>\n",
       "      <td>Series A</td>\n",
       "      <td>15,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>09/02/2021</td>\n",
       "      <td>SplashLearn</td>\n",
       "      <td>EdTech</td>\n",
       "      <td>Game-based learning programme</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>Owl Ventures</td>\n",
       "      <td>Series C</td>\n",
       "      <td>18,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>15/01/2021</td>\n",
       "      <td>Digit Insurance</td>\n",
       "      <td>Financial Services</td>\n",
       "      <td>Insurance Services</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>A91 Partners, Faering Capital, TVS Capital Funds</td>\n",
       "      <td>Venture</td>\n",
       "      <td>1,80,00,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>28/01/2021</td>\n",
       "      <td>Bombay Shaving Company</td>\n",
       "      <td>Consumer Goods Company</td>\n",
       "      <td>Shave care, beard care, and skincare products</td>\n",
       "      <td>New Delhi</td>\n",
       "      <td>Reckitt Benckiser</td>\n",
       "      <td>Venture</td>\n",
       "      <td>6,172,258.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19/01/2021</td>\n",
       "      <td>DeHaat</td>\n",
       "      <td>AgriTech Startup</td>\n",
       "      <td>online marketplace for farm products and services</td>\n",
       "      <td>Patna</td>\n",
       "      <td>Prosus Ventures</td>\n",
       "      <td>Series C</td>\n",
       "      <td>30,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19/01/2021</td>\n",
       "      <td>Darwinbox</td>\n",
       "      <td>SaaS</td>\n",
       "      <td>HR Tech</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Salesforce Ventures</td>\n",
       "      <td>Seed</td>\n",
       "      <td>15,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>18/01/2021</td>\n",
       "      <td>mfine</td>\n",
       "      <td>Health Tech Startup</td>\n",
       "      <td>AI-powered telemedicine mobile app</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Heritas Capital Management</td>\n",
       "      <td>Venture Round</td>\n",
       "      <td>16,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>18/01/2021</td>\n",
       "      <td>Udayy</td>\n",
       "      <td>EdTech</td>\n",
       "      <td>Online learning platform for kids in class 1-5</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>Sequoia Capital</td>\n",
       "      <td>Seed Funding</td>\n",
       "      <td>15,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>11/01/2021</td>\n",
       "      <td>True Elements</td>\n",
       "      <td>Food Startup</td>\n",
       "      <td>Whole Food plant based Nashta</td>\n",
       "      <td>Pune</td>\n",
       "      <td>SIDBI Venture Capital</td>\n",
       "      <td>Series</td>\n",
       "      <td>100,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>13/01/2021</td>\n",
       "      <td>Saveo</td>\n",
       "      <td>B2B E-commerce</td>\n",
       "      <td>Pharmacies</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Matrix Partners India, RTP Global, others</td>\n",
       "      <td>Seed</td>\n",
       "      <td>4,000,000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date            Startup name                Industry  \\\n",
       "0   04/03/2021               DealShare              E-commerce   \n",
       "1   31/03/2021                Uniphore              Technology   \n",
       "2   30/03/2021                   Dunzo              E-commerce   \n",
       "3   30/03/2021                  BYJU’S                Edu-tech   \n",
       "4   23/03/2021             SkilloVilla                Edu-tech   \n",
       "5   25/03/2021                CityMall              E-commerce   \n",
       "6   26/03/2021                   DotPe                 FinTech   \n",
       "7   11/02/2021                Doubtnut                Edu Tech   \n",
       "8   22/02/2021                  Zomato             Hospitality   \n",
       "9   19/02/2021               Fingerlix             Hospitality   \n",
       "10  17/02/2021                   Zolve                 FinTech   \n",
       "11  15/02/2021               KreditBee                 Finance   \n",
       "12  12/02/2021               Pepperfry              E-commerce   \n",
       "13  12/02/2021                 Grofers              E-Commerce   \n",
       "14  09/02/2021                 Nothing              Technology   \n",
       "15  09/02/2021             SplashLearn                  EdTech   \n",
       "16  15/01/2021         Digit Insurance      Financial Services   \n",
       "17  28/01/2021  Bombay Shaving Company  Consumer Goods Company   \n",
       "18  19/01/2021                  DeHaat        AgriTech Startup   \n",
       "19  19/01/2021               Darwinbox                    SaaS   \n",
       "20  18/01/2021                   mfine     Health Tech Startup   \n",
       "21  18/01/2021                   Udayy                  EdTech   \n",
       "22  11/01/2021           True Elements            Food Startup   \n",
       "23  13/01/2021                   Saveo          B2B E-commerce   \n",
       "\n",
       "                                          Subvertical           Location  \\\n",
       "0                            Online shopping platform  Jaipur, Rajasthan   \n",
       "1             Conversational Service Automation (CSA)          Palo Alto   \n",
       "2                            Hyper-local delivery app          Bengaluru   \n",
       "3                                     Online tutoring          Bengaluru   \n",
       "4                 Career and job-oriented upskilling.          Bengaluru   \n",
       "5        Social ecommerce and online grocery platform            Gurgaon   \n",
       "6   Commerce and payments platform to offline ente...            Gurgaon   \n",
       "7                                 E-Learning Platform            Gurgaon   \n",
       "8                       Online Food Delivery Platform            Gurgaon   \n",
       "9                       Semi-cooked food delivery app             Mumbai   \n",
       "10                             Global Neobank Venture             Mumbai   \n",
       "11                           Digital lending platform          Bengaluru   \n",
       "12                        Multi-brand furniture brand             Mumbai   \n",
       "13                                 Online supermarket            Gurgaon   \n",
       "14                        Consumer Technology Venture             London   \n",
       "15                      Game-based learning programme            Gurgaon   \n",
       "16                                 Insurance Services          Bengaluru   \n",
       "17      Shave care, beard care, and skincare products          New Delhi   \n",
       "18  online marketplace for farm products and services              Patna   \n",
       "19                                            HR Tech             Mumbai   \n",
       "20                 AI-powered telemedicine mobile app          Bengaluru   \n",
       "21     Online learning platform for kids in class 1-5            Gurgaon   \n",
       "22                      Whole Food plant based Nashta               Pune   \n",
       "23                                         Pharmacies          Bengaluru   \n",
       "\n",
       "                                        Investor name Investment tpye  \\\n",
       "0                                     Innoven Capital  Debt Financing   \n",
       "1                           Sorenson Capital Partners        Series D   \n",
       "2                           Krishtal Advisors Pte Ltd        Series E   \n",
       "3          MC Global Edtech, B Capital, Baron, others        Series F   \n",
       "4                               Titan Capital, others            Seed   \n",
       "5                                      Accel Partners        Series A   \n",
       "6                                                PayU        Series A   \n",
       "7   SIG Global, Sequoia Capital, WaterBridge Ventu...        Series B   \n",
       "8                                  Tiger Global, Kora         Venture   \n",
       "9            Rhodium Trust, Accel Partners and Swiggy        Series C   \n",
       "10     Accel Partners and Lightspeed Venture Partners            Seed   \n",
       "11  Azim Premji’s PremjiInvest and South Korea’s M...        Series C   \n",
       "12                                    InnoVen Capital  Debt Financing   \n",
       "13                         SoftBank Vision Fund (SVF)     Unspecified   \n",
       "14                                                 GV        Series A   \n",
       "15                                       Owl Ventures        Series C   \n",
       "16   A91 Partners, Faering Capital, TVS Capital Funds         Venture   \n",
       "17                                  Reckitt Benckiser         Venture   \n",
       "18                                    Prosus Ventures        Series C   \n",
       "19                                Salesforce Ventures            Seed   \n",
       "20                         Heritas Capital Management   Venture Round   \n",
       "21                                    Sequoia Capital    Seed Funding   \n",
       "22                              SIDBI Venture Capital          Series   \n",
       "23          Matrix Partners India, RTP Global, others            Seed   \n",
       "\n",
       "   Amount(in USD)  \n",
       "0     250,000,000  \n",
       "1     140,000,000  \n",
       "2       8,000,000  \n",
       "3     460,000,000  \n",
       "4     300,000,000  \n",
       "5      11,000,000  \n",
       "6      27,500,000  \n",
       "7       2,500,000  \n",
       "8     250,000,000  \n",
       "9    2,747,045.20  \n",
       "10    1,50,00,000  \n",
       "11     75,000,000  \n",
       "12      4,773,958  \n",
       "13     55,000,000  \n",
       "14     15,000,000  \n",
       "15     18,000,000  \n",
       "16    1,80,00,000  \n",
       "17   6,172,258.50  \n",
       "18     30,000,000  \n",
       "19     15,000,000  \n",
       "20     16,000,000  \n",
       "21     15,000,000  \n",
       "22    100,000,000  \n",
       "23      4,000,000  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funding_df = pd.DataFrame({})\n",
    "funding_df['Date'] = date\n",
    "funding_df['Startup name'] = startup_name\n",
    "funding_df['Industry'] = industry\n",
    "funding_df['Subvertical'] = subvertical\n",
    "funding_df['Location'] = loc\n",
    "funding_df['Investor name'] = investor_name\n",
    "funding_df['Investment tpye'] = investment_type\n",
    "funding_df['Amount(in USD)'] = amt\n",
    "\n",
    "funding_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5484592",
   "metadata": {},
   "source": [
    "### 7. Write a program to scrap all the available details of best gaming laptops from digit.in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1e8e16f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.digit.in/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e448e172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no data\n"
     ]
    }
   ],
   "source": [
    "#clicking on top 10 tab\n",
    "try:\n",
    "    top10=driver.find_element_by_xpath(\"/html/body/div[2]/div/div[4]/ul/li[4]/a\")\n",
    "    top10.click()\n",
    "except:\n",
    "    print('no data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "820276fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no data\n"
     ]
    }
   ],
   "source": [
    "#clicking on laptops option\n",
    "\n",
    "try:\n",
    "    laptop=driver.find_element_by_xpath(\"/html/body/div[2]/div/div[4]/ul/li[3]/a\")\n",
    "except:\n",
    "    print('no data')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a1492f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no data\n"
     ]
    }
   ],
   "source": [
    "#best gaming laptops link\n",
    "try:\n",
    "    best_gaming=driver.find_element_by_xpath(\"/html/body/div[7]/div/div[2]/div[2]/ul/li[10]/a\")\n",
    "    driver.get(best_gaming.get_attribute('href'))\n",
    "except:\n",
    "    print('no data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "065f0f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Empty List\n",
    "\n",
    "Gaming_Laptops = {}\n",
    "Gaming_Laptops['Laptop_name'] = []\n",
    "Gaming_Laptops['OS'] = []\n",
    "Gaming_Laptops['display'] = []\n",
    "Gaming_Laptops['processor'] = []\n",
    "Gaming_Laptops['HDD'] = []\n",
    "Gaming_Laptops['RAM'] = []\n",
    "Gaming_Laptops['weight'] = []\n",
    "Gaming_Laptops['dimension'] = []\n",
    "Gaming_Laptops['GPU'] = []\n",
    "Gaming_Laptops['price'] = []\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "# Scraping names\n",
    "names=driver.find_elements_by_xpath(\"//div[@class='right-container']/div/a/h3\")\n",
    "for i in names:\n",
    "    Gaming_Laptops['Laptop_name'].append(i.text)\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "# Scraping operating system\n",
    "os=driver.find_elements_by_xpath(\"//div[@class='product-detail']/div/ul/li[1]/div/div\")\n",
    "for i in os:\n",
    "    Gaming_Laptops['OS'].append(i.text)\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "# Scraping display\n",
    "displays=driver.find_elements_by_xpath(\"//div[@class='product-detail']/div/ul/li[2]/div/div\")\n",
    "for i in displays:\n",
    "    Gaming_Laptops['display'].append(i.text)\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "# Scraping processor\n",
    "processors=driver.find_elements_by_xpath(\"//div[@class='value']\")\n",
    "for i in processors:\n",
    "    Gaming_Laptops['processor'].append(i.text)\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "# Scraping memory\n",
    "memories=driver.find_elements_by_xpath(\"//div[@class='Spcs-details'][1]/table/tbody/tr[6]/td[3]\")# extrat HDD and RAM form xpath\n",
    "for i in memories:\n",
    "    Gaming_Laptops['HDD'].append(i.text.split(\"/\")[0])\n",
    "    Gaming_Laptops['RAM'].append(i.text.split(\"/\")[1])\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "# Scraping weight\n",
    "weights=driver.find_elements_by_xpath(\"//div[@class='Spcs-details'][1]/table/tbody/tr[7]/td[3]\")# extrat weight form xpath\n",
    "for i in weights:\n",
    "    Gaming_Laptops['weight'].append(i.text)\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "# Scraping dimension\n",
    "dimension=[]\n",
    "dimensions=driver.find_elements_by_xpath(\"//div[@class='Spcs-details'][1]/table/tbody/tr[8]/td[3]\") \n",
    "for i in dimensions:\n",
    "    Gaming_Laptops['dimension'].append(i.text)\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "# Scraping graphical processor\n",
    "GPUs=driver.find_elements_by_xpath(\"//div[@class='Spcs-details'][1]/table/tbody/tr[9]/td[3]\") \n",
    "for i in GPUs:\n",
    "    Gaming_Laptops['GPU'].append(i.text)\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "#Scraping price\n",
    "price=driver.find_elements_by_xpath(\"//table[@id='summtable']//tr//td[3]\")\n",
    "for i in price:\n",
    "    Gaming_Laptops['price'].append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4cf70a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gaming_Laptops['processor'] = Gaming_Laptops['processor'][2:40:4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "6b328303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Laptop_name</th>\n",
       "      <th>OS</th>\n",
       "      <th>display</th>\n",
       "      <th>processor</th>\n",
       "      <th>HDD</th>\n",
       "      <th>RAM</th>\n",
       "      <th>weight</th>\n",
       "      <th>dimension</th>\n",
       "      <th>GPU</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Laptop_name, OS, display, processor, HDD, RAM, weight, dimension, GPU, price]\n",
       "Index: []"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Make data frame\n",
    "Gaming_Laptops=pd.DataFrame(Gaming_Laptops)\n",
    "Gaming_Laptops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a22846",
   "metadata": {},
   "source": [
    "### 8. Write a python program to scrape the details for all billionaires from www.forbes.com. Details to bescrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a835ffe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.forbes.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "f7f47cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clicking the explore button\n",
    "button9 = driver.find_element_by_xpath(\"//button[@class='icon--hamburger']\")\n",
    "button9.click()\n",
    "\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "aa1394ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select billioners \n",
    "billioners = driver.find_element_by_xpath(\"/html/body/div[1]/header/nav/div[3]/ul/li[1]\")\n",
    "billioners.click()\n",
    "\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "3b169cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select world billioners \n",
    "world_billioners= driver.find_element_by_xpath(\"/html/body/div[1]/header/nav/div[3]/ul/li[1]/div[2]/ul/li[2]/a\")\n",
    "world_billioners.click()\n",
    "\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "4e937e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank=[]\n",
    "for i in driver.find_elements_by_xpath('//div[@ class=\"rank\"]'):\n",
    "    rank.append(i.text.replace('\\n',''))\n",
    "    \n",
    "len(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "b726abd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Name=[]\n",
    "for i in driver.find_elements_by_xpath('//div[@ class=\"personName\"]'):\n",
    "    Name.append(i.text.replace('\\n',''))\n",
    "    \n",
    "len(Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "6f8ef8f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Net_worth=[]\n",
    "for i in driver.find_elements_by_xpath('//div[@ class=\"netWorth\"]'):\n",
    "    Net_worth.append(i.text.replace('\\n',''))\n",
    "    \n",
    "len(Net_worth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "e74a8b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Country=[]\n",
    "for i in driver.find_elements_by_xpath('//div[@ class=\"countryOfCitizenship\"]'):\n",
    "    Country.append(i.text.replace('\\n',''))\n",
    "    \n",
    "len(Country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "cdf8b278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source=[]\n",
    "for i in driver.find_elements_by_xpath('//div[@ class=\"source-column\"]'):\n",
    "    source.append(i.text.replace('\\n',''))\n",
    "\n",
    "len(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d9ce89b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Industry=[]\n",
    "for i in driver.find_elements_by_xpath('//div[@ class=\"category\"]'):\n",
    "    Industry.append(i.text.replace('\\n',''))\n",
    "    \n",
    "len(Industry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "10326bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Age=[]\n",
    "for i in driver.find_elements_by_xpath('//div[@ class=\"age\"]'):\n",
    "    Age.append(i.text.replace('\\n',''))\n",
    "    \n",
    "len(Age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "d2afd5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "        \n",
    "try:\n",
    "  for _ in range(13):\n",
    "    next = driver.find_element_by_xpath('//button[@class=\"pagination-btn pagination-btn--next \"]')\n",
    "    next.click()\n",
    "    for i in driver.find_elements_by_xpath('//div[@ class=\"rank\"]'):\n",
    "        rank.append(i.text.replace('\\n',''))\n",
    "    for i in driver.find_elements_by_xpath('//div[@ class=\"personName\"]'):\n",
    "        Name.append(i.text.replace('\\n',''))\n",
    "    for i in driver.find_elements_by_xpath('//div[@ class=\"netWorth\"]'):\n",
    "        Net_worth.append(i.text.replace('\\n',''))\n",
    "    for i in driver.find_elements_by_xpath('//div[@ class=\"countryOfCitizenship\"]'):\n",
    "        Country.append(i.text.replace('\\n',''))\n",
    "    for i in driver.find_elements_by_xpath('//div[@ class=\"source-column\"]'):\n",
    "        source.append(i.text.replace('\\n',''))\n",
    "    for i in driver.find_elements_by_xpath('//div[@ class=\"category\"]'):\n",
    "        Industry.append(i.text.replace('\\n',''))\n",
    "    for i in driver.find_elements_by_xpath('//div[@ class=\"age\"]'):\n",
    "        Age.append(i.text.replace('\\n',''))\n",
    "        \n",
    "except:\n",
    "    print('no data')\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "453998c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0 0 0 0 0\n"
     ]
    }
   ],
   "source": [
    "print(len(rank),len(Name),len(Net_worth),len(Country),len(source),len(Industry),len(Age))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ad538ade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Name</th>\n",
       "      <th>Net_worth</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Source</th>\n",
       "      <th>Citizenship</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Rank, Name, Net_worth, Industry, Source, Citizenship, Age]\n",
       "Index: []"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Billiniore=pd.DataFrame({})\n",
    "\n",
    "Billiniore['Rank']=rank\n",
    "Billiniore['Name']=Name\n",
    "Billiniore['Net_worth']=Net_worth\n",
    "Billiniore['Industry']=Industry\n",
    "Billiniore['Source']=source\n",
    "Billiniore['Citizenship']=Country\n",
    "Billiniore['Age']=Age\n",
    "\n",
    "Billiniore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd1838d",
   "metadata": {},
   "source": [
    "## 9. Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted from any YouTube Video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "322d9a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.youtube.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "3a63a6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "while(i<100):\n",
    "    driver.execute_script(\"window.scrollBy(0,500)\") # scroll down to get more comments\n",
    "    i+=1\n",
    "while(i<402):\n",
    "    driver.execute_script(\"window.scrollBy(0,5000)\") # scroll down to get more comments\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "0d9c09d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmt = []\n",
    "upvt = []\n",
    "cmttime = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "7bc73eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmt_tag = driver.find_elements_by_xpath(\"//*[@id='content-text']\")\n",
    "for i in cmt_tag:\n",
    "    cmt.append(i.text.replace(\"\\n\",''))\n",
    "\n",
    "upvt_tag = driver.find_elements_by_xpath(\"//*[@id='vote-count-middle']\")\n",
    "for j in upvt_tag:\n",
    "    try: \n",
    "        upvt.append(j.text)\n",
    "    except:\n",
    "        upvt.append('-')\n",
    "cmttime_tag = driver.find_elements_by_xpath(\"//*[@id='header-author']/yt-formatted-string/a\")\n",
    "for k in cmttime_tag:\n",
    "    try: \n",
    "        cmttime.append(k.text)\n",
    "    except:\n",
    "        cmttime.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "8d39af04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(upvt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "63ee3e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "4e5355b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cmttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "ae3106c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(upvt)):\n",
    "    if upvt[i] == '': \n",
    "        upvt[i] = '-' # replacing empty string with '-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "621af272",
   "metadata": {},
   "outputs": [],
   "source": [
    "Youtube = {'Comment':cmt,'Commented':cmttime,'Number of upvotes':upvt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "badb6d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "YoutubeDF = pd.DataFrame(Youtube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "dbcc77a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Commented</th>\n",
       "      <th>Number of upvotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Comment, Commented, Number of upvotes]\n",
       "Index: []"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YoutubeDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f1fbc0",
   "metadata": {},
   "source": [
    "## 10. Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in “London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall reviews, privates from price, dorms from price, facilities and property description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "7831275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\Users\\Thamizh\\Downloads\\chromedriver_win32 (2)\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "ca1d8ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.hostelworld.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "cd3cec8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locating the location search bar\n",
    "search_bar = driver.find_element_by_xpath(\"//input[@class='location-text']\")\n",
    "search_bar.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "b2d37015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entering Lonodn in search bar\n",
    "search_bar.send_keys('london')\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "77901237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do click on search button\n",
    "search_but = driver.find_element_by_xpath(\"//button[@id='search-button']\")\n",
    "search_but.click()\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "402a0598",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets find required data\n",
    "hostel_name = []\n",
    "distance = []\n",
    "pvt_prices = []\n",
    "dorms_price = []\n",
    "rating = []\n",
    "reviews = []\n",
    "over_all = []\n",
    "facilities = []\n",
    "description =[]\n",
    "product_url = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "34bc72bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping the required informations\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class = 'pagination-item pagination-current' or @class='pagination-item']\"):\n",
    "    i.click()\n",
    "    time.sleep(4)\n",
    "    \n",
    "# Scraping hostel name\n",
    "    try:\n",
    "        name = driver.find_elements_by_xpath(\"//h2[@class='title title-6']\")\n",
    "        for i in name:\n",
    "            hostel_name.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        hostel_name.append('-')\n",
    "        \n",
    "# Scraping distance from city centre\n",
    "    try:\n",
    "        dist = driver.find_elements_by_xpath(\"//div[@class='subtitle body-3']//a//span[1]\")\n",
    "        for i in dist:\n",
    "            distance.append(i.text.replace('Hostel - ',''))\n",
    "    except NoSuchElementException:\n",
    "        distance.append('-')\n",
    "        \n",
    "    for i in driver.find_elements_by_xpath(\"//div[@class='prices-col']\"):\n",
    "# Scraping privates from price\n",
    "        try:\n",
    "            pvt_price = driver.find_element_by_xpath(\"//a[@class='prices']//div[1]//div\")\n",
    "            pvt_prices.append(pvt_price.text)\n",
    "        except NoSuchElementException:\n",
    "            pvt_prices.append('-')\n",
    "            \n",
    "# Scraping dorms from price\n",
    "    for i in driver.find_elements_by_xpath(\"//div[@class='prices-col']\"):\n",
    "        try:\n",
    "            dorms = driver.find_element_by_xpath(\"//a[@class='prices']//div[2]//div\")\n",
    "            dorms_price.append(dorms.text)\n",
    "        except NoSuchElementException:\n",
    "            dorms_price.append('-')\n",
    "            \n",
    "# Scraping facilities\n",
    "    try:\n",
    "        fac1 = driver.find_elements_by_xpath(\"//div[@class='has-wifi']\")\n",
    "        fac2 = driver.find_elements_by_xpath(\"//div[@class='has-sanitation']\")\n",
    "        for i in fac1:\n",
    "            for j in fac2:\n",
    "                facilities.append(i.text +', '+ j.text )\n",
    "    except NoSuchElementException:\n",
    "        facilities.append('-')\n",
    "        \n",
    "# Fetching url of each hostel\n",
    "    p_url = driver.find_elements_by_xpath(\"//div[@class='prices-col']//a[2]\")\n",
    "    for i in p_url:\n",
    "        product_url.append(i.get_attribute('href'))\n",
    "\n",
    "for i in product_url:\n",
    "    driver.get(i)\n",
    "    time.sleep(3)\n",
    "# Do click on show more button for description\n",
    "    try:\n",
    "        driver.find_element_by_xpath(\"//a[@class='toggle-content']\").click()\n",
    "        time.sleep(5)\n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "    \n",
    "# Scraping ratings\n",
    "    try:\n",
    "        rat = driver.find_element_by_xpath(\"//div[@class='score orange big' or @class='score gray big']\")\n",
    "        rating.append(rat.text)\n",
    "    except NoSuchElementException:\n",
    "        rating.append('-')\n",
    "        \n",
    "# Scraping total reviews   \n",
    "    try:\n",
    "        rws = driver.find_element_by_xpath(\"//div[@class='reviews']\")\n",
    "        reviews.append(rws.text.replace('Total Reviews',''))\n",
    "    except NoSuchElementException:\n",
    "        reviews.append('-')\n",
    "        \n",
    "# Fetching overall review\n",
    "    try:\n",
    "        overall_rw = driver.find_element_by_xpath(\"//div[@class='keyword']//span\")\n",
    "        over_all.append(overall_rw.text)\n",
    "    except NoSuchElementException:\n",
    "        over_all.append('-')\n",
    "        \n",
    "# Fetching property description \n",
    "    try:\n",
    "        disc = driver.find_element_by_xpath(\"//div[@class='content']\")\n",
    "        description.append(disc.text)\n",
    "    except NoSuchElementException:\n",
    "        over_all.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "932058a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hostel_Name</th>\n",
       "      <th>Distance fron city centre</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Total_reviews</th>\n",
       "      <th>Overall Reviews</th>\n",
       "      <th>Privates from price</th>\n",
       "      <th>Dorms from price</th>\n",
       "      <th>Facilities</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Hostel_Name, Distance fron city centre, Ratings, Total_reviews, Overall Reviews, Privates from price, Dorms from price, Facilities, Description]\n",
       "Index: []"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating dataframe to store scraped data\n",
    "\n",
    "Hostels = pd.DataFrame({})\n",
    "Hostels['Hostel_Name'] = hostel_name[:70]\n",
    "Hostels['Distance fron city centre'] = distance[:70]\n",
    "Hostels['Ratings'] = rating[:70]\n",
    "Hostels['Total_reviews'] = reviews[:70]\n",
    "Hostels['Overall Reviews'] = over_all[:70]\n",
    "Hostels['Privates from price'] = pvt_prices[:70]\n",
    "Hostels['Dorms from price'] = dorms_price[:70]\n",
    "Hostels['Facilities'] = facilities[:70]\n",
    "Hostels['Description'] = description[:70]\n",
    "\n",
    "Hostels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db54e317",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
